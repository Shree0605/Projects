{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZJt4kC897-v",
        "outputId": "d85becd7-8862-44e1-90cf-f6da4218ee20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-yfh3trf1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-yfh3trf1\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Using cached tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Using cached tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803707 sha256=bc900f4c2ed71e964c858926232aa643a4c58f5075afd0c4651a6b1bf3fbf065\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8ih230lw/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install whisper\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install pydub\n",
        "!pip install matplotlib\n",
        "!pip install nltk spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import\n",
        "import whisper\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Step 3: Upload your audio\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "audio_file = list(uploaded.keys())[0]\n",
        "\n",
        "# Step 4: (Optional) Convert to wav if needed\n",
        "if not audio_file.endswith('.wav'):\n",
        "    sound = AudioSegment.from_file(audio_file)\n",
        "    audio_file_wav = audio_file.split('.')[0] + '.wav'\n",
        "    sound.export(audio_file_wav, format=\"wav\")\n",
        "    audio_file = audio_file_wav\n",
        "\n",
        "# Step 5: Transcribe\n",
        "print(\"Transcribing...\")\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(audio_file)\n",
        "transcript = result['text']\n",
        "\n",
        "# Step 6: Show and Save\n",
        "print(\"\\n--- TRANSCRIPT ---\\n\")\n",
        "print(transcript)\n",
        "\n",
        "with open('transcript.txt', 'w') as f:\n",
        "    f.write(transcript)\n",
        "\n",
        "print(\"\\n✅ Transcription done. File saved as transcript.txt.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "BPkQBabm-Cdi",
        "outputId": "1fe5d041-c5ec-45cd-c60c-b040ffa573ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fbb9b519-9a72-47f7-902e-db43124cc261\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fbb9b519-9a72-47f7-902e-db43124cc261\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving prodev.mp3 to prodev (1).mp3\n",
            "Transcribing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 163MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TRANSCRIPT ---\n",
            "\n",
            " John, good morning, everyone. Let's kick off the meeting. We are here to discuss the next steps in the development of our finance management tool for small businesses. The goal is to create a tool that helps small business owners track their income, expenses, and profit with ease. Sarah, sounds great. I think we're at a point where we finalized the basic feature set. The challenge now is ensuring that the product is scalable, secure, and user-friendly. Alex, I agree. From a design perspective, we need to focus on simplicity and accessibility. Small business owners often don't have time to learn complex software. The interface needs to be intuitive. John, exactly, Alex. That's why we want to keep it simple. Let's start by reviewing the features we've already agreed on for the initial release. Sarah, sure. For version one, we're planning these core features. Income and expense tracking. Basic financial reporting, profit and loss, balance sheet. Invoice generation and sending. Bank account integration for automatic transactions syncing. Maria, I think these are great. From the marketing side, I'd suggest focusing on the simplicity of the user interface when we promote the product. Small business owners want tools that save them time, not complicate their processes. David, agreed. I'd also like to point out that security is going to be critical. We need to ensure that financial data is protected at all times. This means implementing strong encryption and two-factor authentication, especially when dealing with bank account integrations. Sarah, absolutely. We're already considering using AES 256 encryption for data storage. For two-factor authentication, I'm leaning towards integrating SMS or email-based authentication for now. It's simple and effective. John, sounds like a solid plan. Moving on to the design, Alex, where are we with the user interface mockups? Alex, I've completed the wireframes for the dashboard, which will give users a quick snapshot of their finances, income, expenses, and profit for the month. I've also designed the invoice generation screen. The goal is to make the UI clean with only the most necessary information visible. Sarah, can we make sure the dashboard is customizable? Some business owners might want to see specific information up front, like inventory levels or outstanding invoices, so I think it's important to allow them to configure what's shown. Alex, that's a great point. I'll add an option for users to customize the widgets on the dashboard. John, excellent. Maria, how do you think we should position the product in the market? Maria, we're targeting small businesses, specifically those in the service and retail sectors. I think we should highlight the ease of use and time-saving aspects. We could offer a free version with basic features and a premium version with advanced reporting and integrations. David, I think that's a good idea. We can also offer trial periods for the premium features, so users.\n",
            "\n",
            "✅ Transcription done. File saved as transcript.txt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load your transcript\n",
        "with open('transcript.txt', 'r') as f:\n",
        "    transcript = f.read()\n",
        "\n",
        "# --- Step 1: Remove Names ---\n",
        "# List of names mentioned\n",
        "def clean_speech_to_text(transcript):\n",
        "    # This removes capitalized names followed by a comma (e.g., \"John,\")\n",
        "    return re.sub(r'\\b[A-Z][a-z]+,', '', transcript)\n",
        "\n",
        "# --- Step 2: Remove Extra Punctuation ---\n",
        "# Remove multiple spaces\n",
        "transcript = re.sub(r'\\s+', ' ', transcript)\n",
        "\n",
        "# Optionally remove other stray punctuations (keeping full stops if you want sentence splits)\n",
        "transcript = re.sub(r'[^a-zA-Z0-9. ]+', '', transcript)\n",
        "\n",
        "# --- Step 3: Save Cleaned Transcript ---\n",
        "with open('cleaned_transcript.txt', 'w') as f:\n",
        "    f.write(transcript)\n",
        "\n",
        "print(\"\\n✅ Cleaned transcript ready and saved as 'cleaned_transcript.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eLV8Fbd-CaO",
        "outputId": "3b703972-fde7-4416-edea-8d6f45b52e81"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Cleaned transcript ready and saved as 'cleaned_transcript.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# --- Load cleaned transcript ---\n",
        "with open('cleaned_transcript.txt', 'r') as f:\n",
        "    cleaned_text = f.read()\n",
        "\n",
        "# --- Step 1: Split text into sentences ---\n",
        "sentences = sent_tokenize(cleaned_text)\n",
        "\n",
        "# --- Step 2: Define casual phrases ---\n",
        "casual_phrases = [\n",
        "    \"good morning\", \"sounds great\", \"i agree\",\"good morning\",\" everyone\",\"Lets kick off\",\" the meeting\", \"exactly\", \"that's great\", \"yeah\",\"um\",\"uh\",\"okay\",\n",
        "    \"solid plan\", \"excellent\", \"moving on\", \"sure\", \"think\", \"point\", \"agreed\"\n",
        "]\n",
        "\n",
        "# --- Step 3: Clean Introduction (first 5 important sentences only) ---\n",
        "intro_sentences = []\n",
        "count = 0\n",
        "i = 0\n",
        "\n",
        "while count < 5 and i < len(sentences):\n",
        "    lower_sent = sentences[i].lower()\n",
        "    if not any(phrase in lower_sent for phrase in casual_phrases):\n",
        "        intro_sentences.append(sentences[i].strip())\n",
        "        count += 1\n",
        "    i += 1\n",
        "\n",
        "introduction = \" \".join(intro_sentences)\n",
        "\n",
        "# --- Step 4: Clean Main Points ---\n",
        "# (no change here from your code)\n",
        "important_sentences = []\n",
        "for sent in sentences[5:]:  # Start after introduction part\n",
        "    lower_sent = sent.lower()\n",
        "    if not any(phrase in lower_sent for phrase in casual_phrases):\n",
        "        important_sentences.append(sent.strip())\n",
        "\n",
        "# --- Make main points into bullets ---\n",
        "main_points = \"\\n\".join([f\"- {s}\" for s in important_sentences])\n",
        "\n",
        "# --- Output ---\n",
        "print(\"\\n--- INTRODUCTION ---\\n\")\n",
        "print(introduction)\n",
        "\n",
        "print(\"\\n--- MAIN POINTS ---\\n\")\n",
        "print(main_points)\n",
        "\n",
        "# --- Save the outputs ---\n",
        "with open('introduction.txt', 'w') as f:\n",
        "    f.write(introduction)\n",
        "\n",
        "with open('main_points.txt', 'w') as f:\n",
        "    f.write(main_points)\n",
        "\n",
        "print(\"\\n✅ Introduction and Main Points generated and saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8vIX8KB-CXy",
        "outputId": "6d9a8942-8885-4e4a-fca6-72f1de9cf448"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- INTRODUCTION ---\n",
            "\n",
            "We are here to discuss the next steps in the development of our finance management tool for small businesses. The goal is to create a tool that helps small business owners track their income expenses and profit with ease. The challenge now is ensuring that the product is scalable secure and userfriendly. From a design perspective we need to focus on simplicity and accessibility. Small business owners often dont have time to learn complex software.\n",
            "\n",
            "--- MAIN POINTS ---\n",
            "\n",
            "- The challenge now is ensuring that the product is scalable secure and userfriendly.\n",
            "- From a design perspective we need to focus on simplicity and accessibility.\n",
            "- Small business owners often dont have time to learn complex software.\n",
            "- The interface needs to be intuitive.\n",
            "- Thats why we want to keep it simple.\n",
            "- For version one were planning these core features.\n",
            "- Income and expense tracking.\n",
            "- Basic financial reporting profit and loss balance sheet.\n",
            "- Invoice generation and sending.\n",
            "- Bank account integration for automatic transactions syncing.\n",
            "- From the marketing side Id suggest focusing on the simplicity of the user interface when we promote the product.\n",
            "- Small business owners want tools that save them time not complicate their processes.\n",
            "- This means implementing strong encryption and twofactor authentication especially when dealing with bank account integrations.\n",
            "- Sarah absolutely.\n",
            "- Were already considering using AES 256 encryption for data storage.\n",
            "- For twofactor authentication Im leaning towards integrating SMS or emailbased authentication for now.\n",
            "- Its simple and effective.\n",
            "- Ive also designed the invoice generation screen.\n",
            "- The goal is to make the UI clean with only the most necessary information visible.\n",
            "- Ill add an option for users to customize the widgets on the dashboard.\n",
            "\n",
            "✅ Introduction and Main Points generated and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# --- Load cleaned transcript ---\n",
        "with open('cleaned_transcript.txt', 'r') as f:\n",
        "    cleaned_text = f.read()\n",
        "\n",
        "# --- Step 1: Split text into sentences ---\n",
        "sentences = sent_tokenize(cleaned_text)\n",
        "\n",
        "# --- Step 2: Smarter Action Items Extraction ---\n",
        "# Only match real task verbs\n",
        "important_action_verbs = [\n",
        "    \"complete\", \"implement\", \"design\", \"finalize\", \"add\",\n",
        "    \"develop\", \"prepare\", \"customize\", \"build\", \"review\"\n",
        "]\n",
        "\n",
        "action_items = []\n",
        "\n",
        "for sent in sentences:\n",
        "    lower_sent = sent.lower()\n",
        "    # Must have an action verb, but should not start with casual conversation\n",
        "    if any(verb in lower_sent for verb in important_action_verbs):\n",
        "        if not lower_sent.startswith(('we are', 'we need', 'i think', 'sounds like', 'moving on', 'lets start', 'agreed', 'sure')):\n",
        "            action_items.append(sent.strip())\n",
        "\n",
        "# --- Step 3: Deadlines inside action items ---\n",
        "deadline_keywords = [\"by\", \"before\", \"next week\", \"next meeting\", \"end of\", \"within\", \"upcoming\"]\n",
        "\n",
        "final_action_items = []\n",
        "for sent in action_items:\n",
        "    deadline = \"No specific deadline\"\n",
        "    for keyword in deadline_keywords:\n",
        "        if keyword in sent.lower():\n",
        "            deadline = \"Deadline mentioned: \" + keyword\n",
        "            break\n",
        "    final_action_items.append(f\"- {sent} ({deadline})\")\n",
        "\n",
        "# Handle if no action items found\n",
        "if not final_action_items:\n",
        "    final_action_items = [\"No action items found.\"]\n",
        "\n",
        "# --- Step 4: Extra Notes Extraction ---\n",
        "future_keywords = [\"next meeting\", \"future discussion\", \"we will discuss\", \"upcoming topics\", \"plan to discuss\"]\n",
        "\n",
        "extra_notes = []\n",
        "for sent in sentences:\n",
        "    lower_sent = sent.lower()\n",
        "    if any(keyword in lower_sent for keyword in future_keywords):\n",
        "        extra_notes.append(sent.strip())\n",
        "\n",
        "# Handle if no extra notes found\n",
        "if not extra_notes:\n",
        "    extra_notes = [\"No extra notes found.\"]\n",
        "\n",
        "# --- Step 5: Save Action Items and Extra Notes into files ---\n",
        "with open('action_items.txt', 'w') as f:\n",
        "    for item in final_action_items:\n",
        "        f.write(item + '\\n')\n",
        "\n",
        "with open('extra_notes.txt', 'w') as f:\n",
        "    for note in extra_notes:\n",
        "        f.write(note + '\\n')\n",
        "\n",
        "# --- Step 6: Print them nicely ---\n",
        "print(\"\\n--- SMART ACTION ITEMS ---\\n\")\n",
        "for item in final_action_items:\n",
        "    print(item)\n",
        "\n",
        "print(\"\\n--- EXTRA NOTES ---\\n\")\n",
        "for note in extra_notes:\n",
        "    print(note)\n",
        "\n",
        "print(\"\\n✅ Action Items and Extra Notes saved and printed successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBf759Io-CSq",
        "outputId": "579bf2dc-aa6f-4037-9de2-02464ad60e35"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SMART ACTION ITEMS ---\n",
            "\n",
            "- From a design perspective we need to focus on simplicity and accessibility. (No specific deadline)\n",
            "- This means implementing strong encryption and twofactor authentication especially when dealing with bank account integrations. (No specific deadline)\n",
            "- Ive also designed the invoice generation screen. (No specific deadline)\n",
            "- Ill add an option for users to customize the widgets on the dashboard. (No specific deadline)\n",
            "\n",
            "--- EXTRA NOTES ---\n",
            "\n",
            "No extra notes found.\n",
            "\n",
            "✅ Action Items and Extra Notes saved and printed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load your smart action items ---\n",
        "with open('action_items.txt', 'r') as f:\n",
        "    action_items_list = f.readlines()\n",
        "\n",
        "# --- Priority Detection Rules ---\n",
        "high_priority_words = ['immediately', 'urgent', 'critical', 'asap', 'important now', 'must']\n",
        "medium_priority_words = ['soon', 'next week', 'plan', 'important', 'upcoming']\n",
        "\n",
        "# --- Function to Assign Priority ---\n",
        "def get_priority(sentence):\n",
        "    lower_sent = sentence.lower()\n",
        "    if any(word in lower_sent for word in high_priority_words):\n",
        "        return \"[High Priority]\"\n",
        "    elif any(word in lower_sent for word in medium_priority_words):\n",
        "        return \"[Medium Priority]\"\n",
        "    else:\n",
        "        return \"[Low Priority]\"\n",
        "\n",
        "# --- Add Priority Tags ---\n",
        "priority_action_items = []\n",
        "for item in action_items_list:\n",
        "    priority = get_priority(item)\n",
        "    priority_action_items.append(f\"{priority} {item.strip()}\")\n",
        "\n",
        "# --- Save into New File ---\n",
        "with open('priority_action_items.txt', 'w') as f:\n",
        "    for item in priority_action_items:\n",
        "        f.write(item + '\\n')\n",
        "\n",
        "# --- Print nicely ---\n",
        "print(\"\\n--- ACTION ITEMS WITH PRIORITY ---\\n\")\n",
        "for item in priority_action_items:\n",
        "    print(item)\n",
        "\n",
        "print(\"\\n✅ Priority tagging completed and saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF0k3x4-CTy1",
        "outputId": "f79de6cb-6f6e-4d7a-e9a9-1f30c9d77487"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ACTION ITEMS WITH PRIORITY ---\n",
            "\n",
            "[Low Priority] - From a design perspective we need to focus on simplicity and accessibility. (No specific deadline)\n",
            "[Low Priority] - This means implementing strong encryption and twofactor authentication especially when dealing with bank account integrations. (No specific deadline)\n",
            "[Low Priority] - Ive also designed the invoice generation screen. (No specific deadline)\n",
            "[Low Priority] - Ill add an option for users to customize the widgets on the dashboard. (No specific deadline)\n",
            "\n",
            "✅ Priority tagging completed and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load full cleaned transcript ---\n",
        "with open('cleaned_transcript.txt', 'r') as f:\n",
        "    full_meeting_text = f.read()\n",
        "\n",
        "# --- Simple Mood Detection Rules ---\n",
        "positive_words = ['good', 'great', 'excellent', 'happy', 'excited', 'awesome']\n",
        "negative_words = ['problem', 'issue', 'difficult', 'frustrated', 'delay', 'confused']\n",
        "\n",
        "positive_count = 0\n",
        "negative_count = 0\n",
        "\n",
        "# Count words\n",
        "for word in positive_words:\n",
        "    positive_count += full_meeting_text.lower().count(word)\n",
        "\n",
        "for word in negative_words:\n",
        "    negative_count += full_meeting_text.lower().count(word)\n",
        "\n",
        "# Decide Mood\n",
        "if positive_count > negative_count:\n",
        "    meeting_mood = \"Overall Mood: Positive\"\n",
        "elif negative_count > positive_count:\n",
        "    meeting_mood = \"Overall Mood: Negative\"\n",
        "else:\n",
        "    meeting_mood = \"Overall Mood: Neutral\"\n",
        "\n",
        "# --- Save Mood ---\n",
        "with open('meeting_mood.txt', 'w') as f:\n",
        "    f.write(meeting_mood)\n",
        "\n",
        "# --- Print nicely ---\n",
        "print(\"\\n--- MEETING MOOD DETECTED ---\\n\")\n",
        "print(meeting_mood)\n",
        "\n",
        "print(\"\\n✅ Mood detection completed and saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg1WPV6fCTpD",
        "outputId": "2469e04e-3bd5-4e51-de12-236bec35ed3a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- MEETING MOOD DETECTED ---\n",
            "\n",
            "Overall Mood: Positive\n",
            "\n",
            "✅ Mood detection completed and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Upload your audio again ---\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "audio_file = list(uploaded.keys())[0]\n",
        "\n",
        "# --- Load Whisper Model ---\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# --- Transcribe with timestamps ---\n",
        "result = model.transcribe(audio_file, word_timestamps=False)  # We want segment timestamps\n",
        "\n",
        "segments = result['segments']\n",
        "\n",
        "# --- Extract start time and text ---\n",
        "times = []\n",
        "topics = []\n",
        "\n",
        "for seg in segments:\n",
        "    start_time = seg['start'] / 60  # convert seconds to minutes\n",
        "    text = seg['text'].strip()\n",
        "\n",
        "    # Simple summarization: pick a keyword (first few words)\n",
        "    if len(text) > 30:\n",
        "        topic = text[:30] + \"...\"\n",
        "    else:\n",
        "        topic = text\n",
        "\n",
        "    times.append(start_time)\n",
        "    topics.append(topic)\n",
        "\n",
        "# --- Create Timeline Chart ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(times, [i for i in range(len(times))], marker='o', linestyle='--')\n",
        "plt.yticks(range(len(topics)), topics)\n",
        "plt.xlabel('Time (minutes)')\n",
        "plt.ylabel('Discussion Points')\n",
        "plt.title('Meeting Discussion Timeline')\n",
        "plt.grid(True)\n",
        "\n",
        "# Save the figure\n",
        "timeline_chart_file = \"timeline_chart.png\"\n",
        "plt.savefig(timeline_chart_file)\n",
        "plt.close()\n",
        "\n",
        "print(\"\\n✅ Timeline Chart generated and saved as 'timeline_chart.png'.\")\n",
        "files.download(timeline_chart_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "fvuVVvkjCtIh",
        "outputId": "49ce9464-bf83-47ca-f229-0a65ef4ae8aa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-40a2c78d-b21a-4d94-b256-9bc1e3d4eaf7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-40a2c78d-b21a-4d94-b256-9bc1e3d4eaf7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving prodev.mp3 to prodev (2).mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Timeline Chart generated and saved as 'timeline_chart.png'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fb7f146d-b79a-4aa5-bc5f-535691a55123\", \"timeline_chart.png\", 137113)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install nltk\n",
        "\n",
        "# --- Import libraries ---\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# --- Load cleaned or raw transcript ---\n",
        "with open('transcript.txt', 'r') as f:   # Using raw transcript because we need speaker names\n",
        "    raw_text = f.read()\n",
        "\n",
        "# --- Step 1: Detect Speakers and Split Sentences ---\n",
        "sentences = sent_tokenize(raw_text)\n",
        "\n",
        "# Collect text spoken by each speaker\n",
        "speaker_texts = defaultdict(str)\n",
        "\n",
        "for sent in sentences:\n",
        "    match = re.match(r\"(\\w+),\", sent)  # Find word ending with comma (name)\n",
        "    if match:\n",
        "        speaker = match.group(1)\n",
        "        # Remove speaker name from sentence\n",
        "        clean_sent = re.sub(r\"^\\w+,\", \"\", sent).strip()\n",
        "        speaker_texts[speaker] += \" \" + clean_sent\n",
        "    else:\n",
        "        # If no name found, assign to \"Unknown\"\n",
        "        speaker_texts[\"Colab\"] += \" \" + sent\n",
        "\n",
        "# --- Step 2: Check number of speakers ---\n",
        "unique_speakers = list(speaker_texts.keys())\n",
        "print(\"\\nSpeakers Detected:\", unique_speakers)\n",
        "\n",
        "# --- Step 3: Create WordCloud(s) ---\n",
        "if len(unique_speakers) == 1:\n",
        "    # Only one speaker - generate normal full meeting wordcloud\n",
        "    text_data = list(speaker_texts.values())[0]\n",
        "    wordcloud = WordCloud(width=1600, height=800, background_color='white').generate(text_data)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Meeting WordCloud', fontsize=20)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    wordcloud_file = \"wordcloud_full_meeting.png\"\n",
        "    plt.savefig(wordcloud_file)\n",
        "    plt.close()\n",
        "    print(\"\\n✅ WordCloud for full meeting saved as 'wordcloud_full_meeting.png'.\")\n",
        "else:\n",
        "    # Multiple speakers - create one wordcloud per speaker\n",
        "    for speaker, text_data in speaker_texts.items():\n",
        "        wordcloud = WordCloud(width=1600, height=800, background_color='white').generate(text_data)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'WordCloud - {speaker}', fontsize=20)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        wordcloud_file = f\"wordcloud_{speaker}.png\"\n",
        "        plt.savefig(wordcloud_file)\n",
        "        plt.close()\n",
        "        print(f\"✅ WordCloud for {speaker} saved as '{wordcloud_file}'.\")\n",
        "\n",
        "print(\"\\n✅ All WordClouds generation done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUysgixy-COi",
        "outputId": "19c2d1b8-b296-44f9-ce90-285d77fee2c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Speakers Detected: ['Colab', 'Sarah', 'Alex', 'John', 'Maria', 'David']\n",
            "✅ WordCloud for Colab saved as 'wordcloud_Colab.png'.\n",
            "✅ WordCloud for Sarah saved as 'wordcloud_Sarah.png'.\n",
            "✅ WordCloud for Alex saved as 'wordcloud_Alex.png'.\n",
            "✅ WordCloud for John saved as 'wordcloud_John.png'.\n",
            "✅ WordCloud for Maria saved as 'wordcloud_Maria.png'.\n",
            "✅ WordCloud for David saved as 'wordcloud_David.png'.\n",
            "\n",
            "✅ All WordClouds generation done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download all saved wordcloud images\n",
        "for speaker in speaker_texts.keys():\n",
        "    filename = f\"wordcloud_{speaker}.png\"\n",
        "    files.download(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "n78O_JflDccx",
        "outputId": "7b56232b-448a-4c4a-a181-8bd89f0176e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c5a34bda-6ab0-416e-a069-511ae5b3af6a\", \"wordcloud_Colab.png\", 437498)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5f26dbb3-c191-4df3-ad61-e3e2d5f381e2\", \"wordcloud_Sarah.png\", 131814)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_51f021f5-03dc-4386-bfe9-0d231cbac45a\", \"wordcloud_Alex.png\", 198498)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e235d79e-b867-402e-9e6d-13ddeedd294b\", \"wordcloud_John.png\", 113669)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_746b303a-5a83-4735-8770-669516223f2a\", \"wordcloud_Maria.png\", 161905)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_036bfec9-ba42-4093-badc-4926f0a9fe45\", \"wordcloud_David.png\", 94735)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install libraries ---\n",
        "!pip install fpdf\n",
        "!pip install pillow\n",
        "!pip install nltk\n",
        "\n",
        "# --- Import libraries ---\n",
        "from fpdf import FPDF\n",
        "from PIL import Image\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# --- Load cleaned transcript ---\n",
        "try:\n",
        "    with open('cleaned_transcript.txt', 'r') as f:\n",
        "        cleaned_text = f.read()\n",
        "except:\n",
        "    cleaned_text = \"Meeting Summary\"\n",
        "\n",
        "# --- Step 1: Generate Dynamic Title ---\n",
        "sentences = sent_tokenize(cleaned_text)\n",
        "\n",
        "if sentences:\n",
        "    first_sentence = sentences[0]\n",
        "    # Try to detect main topic words from the first sentence\n",
        "    match = re.search(r'next steps in the development of (.*?)(?:\\.|$)', first_sentence, re.IGNORECASE)\n",
        "    if match:\n",
        "        meeting_topic = match.group(1).strip()\n",
        "        title_text = f\"Meeting on {meeting_topic}\"\n",
        "    else:\n",
        "        title_text = \"Meeting Summary Report\"\n",
        "else:\n",
        "    title_text = \"Meeting Summary Report\"\n",
        "\n",
        "print(f\"\\n✅ Meeting Title detected: {title_text}\")\n",
        "\n",
        "# --- Create PDF Class ---\n",
        "class PDF(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font('Arial', 'B', 16)\n",
        "        self.cell(0, 10, title_text, ln=True, align='C')\n",
        "        self.ln(10)\n",
        "\n",
        "    def chapter_title(self, title):\n",
        "        self.set_font('Arial', 'B', 14)\n",
        "        self.cell(0, 10, title, ln=True)\n",
        "        self.ln(4)\n",
        "\n",
        "    def chapter_body(self, body):\n",
        "        self.set_font('Arial', '', 12)\n",
        "        self.multi_cell(0, 8, body)\n",
        "        self.ln(5)\n",
        "\n",
        "    def add_image(self, image_path, title, w=180, h=0):\n",
        "        self.add_page()\n",
        "        self.chapter_title(title)\n",
        "        self.image(image_path, x=15, w=w, h=h)\n",
        "        self.ln(5)\n",
        "\n",
        "# --- Create PDF ---\n",
        "pdf = PDF()\n",
        "pdf.set_auto_page_break(auto=True, margin=15)\n",
        "pdf.add_page()\n",
        "\n",
        "# --- Add Text Sections ---\n",
        "def add_text_section(filename, section_title):\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            text = f.read()\n",
        "        pdf.chapter_title(section_title)\n",
        "        pdf.chapter_body(text)\n",
        "    except:\n",
        "        print(f\"Warning: {filename} not found!\")\n",
        "\n",
        "# Add all sections\n",
        "add_text_section('introduction.txt', 'Introduction')\n",
        "add_text_section('main_points.txt', 'Main Points')\n",
        "add_text_section('priority_action_items.txt', 'Action Items with Priority')\n",
        "add_text_section('extra_notes.txt', 'Extra Notes')\n",
        "add_text_section('meeting_mood.txt', 'Meeting Mood')\n",
        "\n",
        "# --- Add Images (Timeline + Wordclouds) ---\n",
        "try:\n",
        "    pdf.add_image('speaker_timeline_chart.png', 'Speaker Timeline Chart')\n",
        "except:\n",
        "    print(\"Warning: Speaker Timeline Chart not found!\")\n",
        "\n",
        "wordcloud_files = [f for f in os.listdir() if f.startswith('wordcloud_') and f.endswith('.png')]\n",
        "\n",
        "for wc_file in wordcloud_files:\n",
        "    speaker_name = wc_file.replace('wordcloud_', '').replace('.png', '')\n",
        "    pdf.add_image(wc_file, f\"Wordcloud - {speaker_name}\")\n",
        "\n",
        "# --- Save PDF ---\n",
        "pdf_file_name = \"Final_Meeting_Report.pdf\"\n",
        "pdf.output(pdf_file_name)\n",
        "\n",
        "print(f\"\\n✅ Final PDF '{pdf_file_name}' created successfully!\")\n",
        "from google.colab import files\n",
        "files.download(pdf_file_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "R-q6fig2DadR",
        "outputId": "fd8f4335-c522-4173-ae93-314f2b98550d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=ec3ac7055c1cb0b220e14c9b41edfc7d97efa6d743279dead01ddcd7b8032bb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Meeting Title detected: Meeting Summary Report\n",
            "Warning: Speaker Timeline Chart not found!\n",
            "\n",
            "✅ Final PDF 'Final_Meeting_Report.pdf' created successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8a68f78b-fd2a-496a-9d12-9b304c51207d\", \"Final_Meeting_Report.pdf\", 1037720)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}